{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalized Book Recommendations - Collabortive Filtering with Polars\n",
    "\n",
    "First, we import the necessary library, Polars, and then load the datasets using `pl.scan_csv()`, which allows us to read large CSV files efficiently and lazily into a [LazyFrame](https://docs.pola.rs/api/python/stable/reference/lazyframe/index.html) object. We then select the relevant columns from each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "books = pl.scan_csv(\"books.csv\").select(\"book_id\", \"title\", \"authors\")\n",
    "ratings = pl.scan_csv(\"ratings.csv\").select(\"user_id\", \"book_id\", \"rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the `books` DataFrame with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.limit().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and also view some examples of the ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.limit().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add our own ratings, either by writing them in `my_ratings.csv` or by putting them in the code as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with my ratings and a custom user id\n",
    "import os\n",
    "if os.path.exists(\"my_ratings.csv\"):\n",
    "    my_ratings = pl.scan_csv(\"my_ratings.csv\")\n",
    "else:\n",
    "    my_ratings = [\n",
    "        [\"The Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy, #1)\", 5],\n",
    "        [\"The Martian\", 5],\n",
    "        [\"Surely You're Joking, Mr. Feynman!: Adventures of a Curious Character\", 5],\n",
    "        ['Going Solo', 5],\n",
    "        [\"Flatland: A Romance of Many Dimensions\", 5],\n",
    "        # Add more books here\n",
    "    ] \n",
    "    my_ratings = pl.LazyFrame(my_ratings, schema=[('title', pl.String), ('my_rating', pl.Int64)])\n",
    "my_ratings = my_ratings.join(books.select(\"book_id\", \"title\"), on='title', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further processing, let us join our ratings with the existing ratings dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.join(my_ratings.select(\"book_id\", \"my_rating\"), how=\"left\", on=\"book_id\")\n",
    "ratings.limit().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering with Polars\n",
    "\n",
    "To predict ratings, we perform the following user-user collaborative filtering steps:\n",
    "\n",
    "**Calculate Common Ratings**: We group the ratings by user ID to determine how many books each user has rated in common with us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_rated_in_common = ratings.group_by(\"user_id\").agg((pl.col(\"my_rating\").is_not_null()).sum().alias(\"articles_rated_in_common\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter by Minimum Common Ratings**: We filter users who have rated at least a specified number of books in common with us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_number_of_books_rated_in_common = 10\n",
    "ratings = ratings.join(articles_rated_in_common, how=\"left\", on=\"user_id\").filter(\n",
    "    pl.col(\"articles_rated_in_common\")>=minimum_number_of_books_rated_in_common)\n",
    "\n",
    "ratings.limit().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Similarity Scores**: We compute the Pearson correlation between our ratings and other users’ ratings to determine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = ratings.group_by(\"user_id\", maintain_order=True).agg(pl.corr(\"rating\", \"my_rating\").alias(\"corr\"))\n",
    "similarities.limit().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter by Minimum Similarity**: We keep only those users whose similarity scores exceed a certain threshold and are not NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_similarity = 0.7\n",
    "ratings = ratings.join(similarities, on=\"user_id\", how=\"left\")\n",
    "ratings = ratings.filter(pl.col(\"corr\")>minimal_similarity)\n",
    "ratings = ratings.filter(pl.col(\"corr\").is_not_nan())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter by Minimum Ratings per Book**: We ensure each book has received a minimum number of ratings to be considered for recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_number_of_ratings = 6\n",
    "ratings_per_article = ratings.group_by(\"book_id\").agg((pl.col(\"rating\").is_not_null()).sum().alias(\"ratings_per_article\"))\n",
    "ratings = ratings.join(ratings_per_article, on=\"book_id\", how=\"left\")\n",
    "ratings = ratings.filter(pl.col(\"ratings_per_article\") >= minimal_number_of_ratings)\n",
    "ratings.limit().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict Ratings**: We predict the ratings for each book by calculating the weighted average of ratings from similar users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_func():\n",
    "    return ((pl.col(\"rating\")*pl.col(\"corr\")).sum())/(pl.col(\"corr\").sum())\n",
    "predictions = ratings.group_by(\"book_id\", maintain_order=True).agg(predict_func().alias(\"prediction\")).select(\"book_id\", \"prediction\")\n",
    "predictions.limit(20).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we join the predicted ratings with the book information and sort them to get the top recommendations. This gives us the following top-20 recommendations with the corresponding predicted rating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.join(books, how=\"left\", on=\"book_id\")\n",
    "predictions = predictions.collect().sort(by=\"prediction\", descending=True).limit(20)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Polars vs Pandas\n",
    "\n",
    "Let us now compare the inference times for this collaborative filtering algorithm implemented in Polars against the same algorithm implemented in Pandas. In order to do so, let us write the collaborative filtering algorithm as a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polars_collaborative_filtering(ratings, my_ratings, num_recs=20, minimal_similarity=0.7, minimal_number_of_ratings=6, minimum_number_of_books_rated_in_common=10):\n",
    "    \"\"\"\n",
    "    Performs collaborative filtering on a dataset of book ratings.\n",
    "\n",
    "    Args:\n",
    "        ratings (DataFrame): The dataset of book ratings.\n",
    "        my_ratings (DataFrame): The user's own ratings.\n",
    "        num_recs (int, optional): The number of recommendations to return. Defaults to 20.\n",
    "        minimal_similarity (float, optional): The minimum similarity threshold between users. Defaults to 0.7.\n",
    "        minimal_number_of_ratings (int, optional): The minimum number of ratings required for a book to be considered. Defaults to 6.\n",
    "        minimum_number_of_books_rated_in_common (int, optional): The minimum number of books rated in common between users. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The predicted ratings for books.\n",
    "\n",
    "    \"\"\"\n",
    "    # Join the ratings dataset with the user's own ratings\n",
    "    ratings = ratings.join(my_ratings.select(\"book_id\", \"my_rating\"), how=\"left\", on=\"book_id\")\n",
    "    \n",
    "    # Calculate the number of articles rated in common for each user\n",
    "    articles_rated_in_common = ratings.group_by(\"user_id\").agg((pl.col(\"my_rating\").is_not_null()).sum().alias(\"articles_rated_in_common\"))\n",
    "    \n",
    "    # Filter out users who have rated less than the minimum number of books in common\n",
    "    ratings = ratings.join(articles_rated_in_common, how=\"left\", on=\"user_id\").filter(\n",
    "        pl.col(\"articles_rated_in_common\") >= minimum_number_of_books_rated_in_common)\n",
    "    \n",
    "    # Calculate the similarity between users based on their ratings\n",
    "    similarities = ratings.group_by(\"user_id\", maintain_order=True).agg(pl.corr(\"rating\", \"my_rating\").alias(\"corr\"))\n",
    "    \n",
    "    # Filter out users whose similarity is below the minimum similarity threshold\n",
    "    ratings = ratings.join(similarities, on=\"user_id\", how=\"left\")\n",
    "    ratings = ratings.filter(pl.col(\"corr\") > minimal_similarity)\n",
    "    ratings = ratings.filter(pl.col(\"corr\").is_not_nan())\n",
    "    \n",
    "    # Calculate the number of ratings per article\n",
    "    ratings_per_article = ratings.group_by(\"book_id\").agg((pl.col(\"rating\").is_not_null()).sum().alias(\"ratings_per_article\"))\n",
    "    \n",
    "    # Filter out articles that have less than the minimum number of ratings\n",
    "    ratings = ratings.join(ratings_per_article, on=\"book_id\", how=\"left\")\n",
    "    ratings = ratings.filter(pl.col(\"ratings_per_article\") >= minimal_number_of_ratings)\n",
    "    \n",
    "    # Define the prediction function\n",
    "    def predict_func():\n",
    "        return ((pl.col(\"rating\") * pl.col(\"corr\")).sum()) / (pl.col(\"corr\").sum())\n",
    "    \n",
    "    # Calculate the predicted ratings for each book\n",
    "    predictions = ratings.group_by(\"book_id\", maintain_order=True).agg(predict_func().alias(\"prediction\")).select(\"book_id\", \"prediction\")\n",
    "\n",
    "    # Return the top recommendations\n",
    "    predictions = predictions.sort(by=\"prediction\", descending=True).limit(num_recs)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "ratings = pl.scan_csv(\"ratings.csv\").select(\"user_id\", \"book_id\", \"rating\")\n",
    "predictions = polars_collaborative_filtering(ratings, my_ratings).collect()\n",
    "predictions = predictions.join(books.collect(), how=\"left\", on=\"book_id\")\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we implement the same collaborative filtering logic using Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def pandas_collaborative_filtering(ratings, my_ratings, num_recs=20, minimal_similarity=0.7, minimal_number_of_ratings=6, minimum_number_of_books_rated_in_common=10):\n",
    "    \"\"\"\n",
    "    Performs collaborative filtering on a dataset of book ratings.\n",
    "\n",
    "    Args:\n",
    "        ratings (DataFrame): The dataset of book ratings.\n",
    "        my_ratings (DataFrame): The user's own ratings.\n",
    "        num_recs (int, optional): The number of recommendations to return. Defaults to 20.\n",
    "        minimal_similarity (float, optional): The minimum similarity threshold between users. Defaults to 0.7.\n",
    "        minimal_number_of_ratings (int, optional): The minimum number of ratings required for a book to be considered. Defaults to 6.\n",
    "        minimum_number_of_books_rated_in_common (int, optional): The minimum number of books rated in common between users. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The predicted ratings for books.\n",
    "\n",
    "    \"\"\"\n",
    "    # Join the ratings dataset with the user's own ratings\n",
    "    ratings = pd.merge(ratings, my_ratings[[\"book_id\", \"my_rating\"]], how=\"left\", on=\"book_id\")\n",
    "\n",
    "    # Calculate the number of articles rated in common for each user\n",
    "    articles_rated_in_common = ratings.groupby(\"user_id\").agg({'my_rating': lambda x: x.notnull().sum()}).rename(columns={\"my_rating\": \"articles_rated_in_common\"})\n",
    "    \n",
    "    # Filter out users who have rated less than the minimum number of books in common\n",
    "    ratings = pd.merge(ratings, articles_rated_in_common, how=\"left\", on=\"user_id\")\n",
    "    ratings = ratings[ratings[\"articles_rated_in_common\"]>=minimum_number_of_books_rated_in_common]\n",
    "    \n",
    "    # Calculate the similarity between users based on their ratings\n",
    "    similarities = ratings.groupby(\"user_id\")[[\"rating\", \"my_rating\"]].corr().unstack().iloc[:, 1].rename(\"corr\")\n",
    "\n",
    "    # Filter out users whose similarity is below the minimum similarity threshold\n",
    "    ratings = ratings.join(similarities, on=\"user_id\", how=\"left\")\n",
    "    ratings = ratings[ratings[\"corr\"]>minimal_similarity]\n",
    "    ratings = ratings[ratings[\"corr\"].notna()]\n",
    "    \n",
    "    # Calculate the number of ratings per article\n",
    "    ratings_per_article = ratings.groupby(\"book_id\").agg({'rating': lambda x: x.notnull().sum()}).rename(columns={\"rating\": \"ratings_per_article\"})\n",
    "    \n",
    "    # Filter out articles that have less than the minimum number of ratings\n",
    "    ratings = pd.merge(ratings, ratings_per_article, on=\"book_id\", how=\"left\")\n",
    "    ratings = ratings[ratings[\"ratings_per_article\"] >= minimal_number_of_ratings]\n",
    "    \n",
    "    # Define the prediction function\n",
    "    def predict_func(x):\n",
    "        return ((x[\"rating\"]*x[\"corr\"]).sum())/(x[\"corr\"].sum())\n",
    "    \n",
    "    # Calculate the predicted ratings for each book\n",
    "    predictions = ratings.groupby(\"book_id\").apply(predict_func).to_frame(\"prediction\")\n",
    "\n",
    "    # Get the top recommendations\n",
    "    predictions = predictions.sort_values(by=\"prediction\", ascending=False).head(num_recs)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compare the execution time of the collaborative filtering process using Polars (both lazy and eager execution) and Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import timeit\n",
    "\n",
    "n = 20\n",
    "polars_time_lazy = timeit(lambda: polars_collaborative_filtering(ratings, my_ratings).collect(), number=n)\n",
    "\n",
    "ratings_df, my_ratings_df = pl.collect_all([ratings, my_ratings])\n",
    "polars_time_eager = timeit(lambda: polars_collaborative_filtering(ratings_df, my_ratings_df), number=n)\n",
    "\n",
    "ratings_pd, my_ratings_pd = ratings.collect().to_pandas(), my_ratings.collect().to_pandas()\n",
    "pandas_time = timeit(lambda: pandas_collaborative_filtering(ratings_pd, my_ratings_pd), number=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this across two different hardware yields the following inference time per iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Polars (lazy): {polars_time_lazy/n:.2f} seconds\")\n",
    "print(f\"Polars (eager): {polars_time_eager/n:.2f} seconds\")\n",
    "print(f\"Pandas: {pandas_time/n:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this on two of my machines yields:\n",
    "\n",
    "**Hardware 1**\n",
    "+ Polars (lazy): \n",
    "  + 1.36 seconds\n",
    "+ Polars (eager): \n",
    "  + 0.30 seconds\n",
    "+ Pandas: \n",
    "  + 13.31 seconds\n",
    "\n",
    "**Hardware 2**\n",
    "+ Polars (lazy): \n",
    "  + 0.54 seconds\n",
    "+ Polars (eager): \n",
    "  + 0.13 seconds\n",
    "+ Pandas: \n",
    "  + 3.03 seconds\n",
    "\n",
    "Interstingly, Polars with lazy execution is 6–10 times faster than Pandas, and in eager execution mode even up to 40 times faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
